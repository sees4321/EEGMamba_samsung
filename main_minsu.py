# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os
from collections import defaultdict
from utils import *
from Data_module import Multi_Task_DataModule
from EEGMamba_samsung import MultiStreamModel
from trainer import train_bin_cls, test_bin_cls
from results_utils import (
    save_subject_curves,
    save_mean_curves_and_subject_acc,
    save_summary_excel,
    save_global_expert_ratio_plots,
    save_task_expert_total_counts,
    print_taskwise_stats,
    process_subject_after_test,
    save_group_expert_ratio_plots,
    save_subject_expert_heatmaps,
    save_taskwise_epoch_mean_curves,
    save_subject_taskwise_loss_curves,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Default setting â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

STREAM_NAMES = ["delta","theta","alpha","lowb","highb","fft","raw","hilb", "hilb_phase", "hilb_freq"]

TASK_NAMES = {
    0: "nback",
    1: "arousal",
    2: "valence",
    3: "stress",
    4: "d2"
}

# â˜… ê¸°ë³¸ ê²°ê³¼ ì €ì¥ root í´ë”
ROOT_BASE_DIR = r"D:\KMS\samsung2024\data\results"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìˆ˜ì • ê°€ëŠ¥í•œ parameter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

CHANNEL_MODE = 0 # # 0 - use all electrode channels, 1 - use Fp(AF7, FPZ, AF8), 2 - use Central (C3, CZ, C4), 3 - Ear (Left, Right). (default: 0)
batch = 32
num_epochs = 300
learning_rate = 5e-4 # 1e-3, 5e-4


# â˜… raw ì „ìš© ì»¤ë„ ë¦¬ìŠ¤íŠ¸ (ì˜ˆì‹œ)
RAW_KERNEL_SIZES = [13] # 25ë‚˜ ë‹¤ë¥¸ ì»¤ë„ë„ ì¶”ê°€ ê°€ëŠ¥ (raw dataì— ëŒ€í•œ tokenizer kernel ë¶€ë¶„ì„)

STREAM_CONFIGS = [
    ["raw","hilb", "fft"], # ["delta","theta","alpha","lowb","highb","fft","raw","hilb", "hilb_phase", "hilb_freq"] ì¤‘ ì•„ë¬´ê±°ë‚˜ ì„ íƒ ê°€ëŠ¥
]

MOE_EXPERT_CANDIDATES = [4] # ì‚¬ìš©í•  expert ìˆ˜ (4,5) ì´ëŸ°ì‹ìœ¼ë¡œ í•´ì„œ forë¬¸ìœ¼ë¡œ ëŒì•„ê°€ê²Œë„ ê°€ëŠ¥

USE_TASK_IDS = [0,1,2,3,4] # ì‚¬ìš©í•  task ("nback": 0, "emotion_arousal": 1, "emotion_valence": 2, "stress": 3, "d2": 4)

# Domain parameter
USE_DANN = True # dann ì‚¬ìš©í•  ì§€
LAMBDA_DA = 0.1
use_entropy_weight = False # entropy ì „ëµ ì‚¬ìš©í• ì§€ (cdan ë•Œë¬¸ì— ë„£ì—ˆì—ˆìŒ)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    num_subj = 49  # ì‹¤í—˜ì— ì°¸ì—¬í•œ ì „ì²´ í”¼í—˜ì ìˆ˜

    seed = 2222
    ManualSeed(seed)

    # â˜… ë°”ê¹¥ ë£¨í”„: stream ì¡°í•©
    for stream_cfg in STREAM_CONFIGS:
        cond_tag = "+".join(stream_cfg)
        print(f"\n========== Stream config: {stream_cfg} ==========\n")

        # â˜… ì•ˆìª½ ë£¨í”„: moe_experts í›„ë³´
        for moe_experts in MOE_EXPERT_CANDIDATES:
            print(f"[CONFIG] streams={stream_cfg}, moe_experts={moe_experts}")


            # ----- ì—¬ê¸°ë¶€í„°ëŠ” ê° (stream_cfg, moe_experts) ì¡°í•©ë§ˆë‹¤ ìƒˆë¡œ ì´ˆê¸°í™” -----

            # ----- ì €ì¥ìš© ë³€ìˆ˜ë“¤ -----
            ts_acc = []  # subjectë³„ ìµœì¢… ì •í™•ë„
            all_tr_acc = []
            all_tr_loss = []
            all_te_acc = []
            all_te_loss = []

            # MoE ì „ì²´ í‰ê· ìš© ë²„í¼
            global_expert_hist = None
            global_token_hist = None
            global_stream_names = None

            per_subj_expert_hist = None # í”¼í—˜ì ë³„ ì €ì¥ ìš©
            per_subj_token_hist = None

            # â˜… subject Ã— task accuracy / sample ìˆ˜ ë²„í¼
            num_tasks = len(TASK_NAMES)
            per_subj_task_acc = np.full((num_subj, num_tasks), np.nan, dtype=float)
            per_subj_task_n = np.zeros((num_subj, num_tasks), dtype=int)

            task_epoch_acc_sum = {t: np.zeros(num_epochs, dtype=float) for t in TASK_NAMES.keys()}
            task_epoch_subj_cnt = {t: np.zeros(num_epochs, dtype=int) for t in TASK_NAMES.keys()}
            task_epoch_loss_sum = {t: np.zeros(num_epochs, dtype=float) for t in TASK_NAMES.keys()}
            task_epoch_loss_subj_cnt = {t: np.zeros(num_epochs, dtype=int) for t in TASK_NAMES.keys()}

            # taskë³„ í‰ê·  ì •í™•ë„ë¥¼ ìœ„í•œ ì „ì—­ ëˆ„ì ê¸°
            global_task_correct = defaultdict(float)  # task tì˜ "ë§ì€ ìƒ˜í”Œ ìˆ˜"
            global_task_total = defaultdict(int)      # task tì˜ ì „ì²´ ìƒ˜í”Œ ìˆ˜

            # â˜… ì‹¤ì œë¡œ ì‚¬ìš©ëœ subject id ëª¨ìŒ
            used_subjects = [] # ë§Œì•½ taskë¡œ stressë‘ n back ê³¨ëëŠ”ë°, íŠ¹ì • subjê°€ ë‘ task ë‹¤ ì—†ìœ¼ë©´ ì—ëŸ¬ ë‚˜ì˜´ -> ì´ëŸ° ì—ëŸ¬ë¥¼ ìœ„í•œ ì½”ë“œ

            # ì‹¤í—˜ë§ˆë‹¤ ë‹¤ë¥¸ base_dirì„ ì‚¬ìš©í•´ì„œ ê²°ê³¼ê°€ ë®ì–´ì“°ì´ì§€ ì•Šê²Œ
            exp_name = f"experts_{moe_experts}_streams_{cond_tag}"
            base_dir = os.path.join(ROOT_BASE_DIR, exp_name)
            os.makedirs(base_dir, exist_ok=True)

            # ============= Subject loop =============
            for subj in range(num_subj):

                Multi_Task_dataset = Multi_Task_DataModule(
                    test_subj=subj,
                    channel_mode=CHANNEL_MODE,
                    batch=batch,
                    use_task_ids=USE_TASK_IDS,
                )
                train_loader = Multi_Task_dataset.train_loader
                test_loader = Multi_Task_dataset.test_loader

                # ğŸ”´ source/test loaderê°€ ë¹„ë©´ ì´ subjëŠ” í†µì§¸ë¡œ ìŠ¤í‚µ
                if len(train_loader) == 0 or len(test_loader) == 0:
                    print(f"[SKIP] subj {subj:02d}: "
                          f"empty loader (source={len(train_loader)}, test={len(test_loader)})")
                    continue

                    # â˜… ì‹¤ì œë¡œ í•™ìŠµ/í‰ê°€ì— ì‚¬ìš©ëœ subjectë§Œ ê¸°ë¡
                used_subjects.append(subj)

                n_ch = len(Multi_Task_dataset.channels)

                # test setì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” task id ìˆ˜ì§‘ (ì •í™•ë„ ë‚´ê¸° ìœ„í•¨)
                valid_task_ids = set()
                for _, task_ids, _, _ in test_loader:
                    valid_task_ids.update(task_ids.tolist())

                task_counts = {} # ê° task ì „ì²´ trialì´ ì–¼ë§ˆë‚˜ ë˜ë‚˜ í™•ì¸ ìš©
                for _, task_ids, _, _ in train_loader:
                    for t in task_ids:
                        t = int(t)
                        if t not in task_counts:
                            task_counts[t] = 0
                        task_counts[t] += 1

                model = MultiStreamModel(
                    in_ch=n_ch,
                    dim=16,
                    dim_2=32, # 1D ìš© feature extractorì—ì„œëŠ” ì‚¬ìš© ì•ˆë¨
                    num_tasks=5,
                    patch_kernel=13,
                    patch_stride=2,
                    feat_depth=1,
                    moe_experts=moe_experts,
                    selected_streams=stream_cfg,   # â˜… í˜„ì¬ stream ì¡°í•© ì‚¬ìš©
                    all_stream_names= STREAM_NAMES,
                    use_dann=USE_DANN,
                    num_domains=num_subj,
                    raw_kernel_sizes=RAW_KERNEL_SIZES,
                ).to(DEVICE)

                train_acc, train_loss, test_acc_hist, test_loss_hist, te_task_acc_hist, te_task_count_hist, te_task_loss_hist = train_bin_cls(
                    model,
                    train_loader=train_loader,
                    test_loader=test_loader,  # DANNìš© target
                    num_epoch=num_epochs,
                    optimizer_name='Adam',
                    learning_rate=str(learning_rate),
                    weight_decay=1e-4,
                    subject_id=subj,
                    valid_task_ids=valid_task_ids,
                    use_dann=USE_DANN,
                    lambda_da=LAMBDA_DA,
                    use_entropy_weight=use_entropy_weight,
                )

                # ì—í¬í¬ë³„ ê¸°ë¡ì„ ì „ì²´ ë²„í¼ì— ìŒ“ê¸°
                all_tr_acc.append(train_acc)
                all_tr_loss.append(train_loss)
                all_te_acc.append(test_acc_hist)
                all_te_loss.append(test_loss_hist)

                # â”€â”€â”€â”€â”€â”€â”€â”€â”€ í”¼í—˜ìë³„ ê³¡ì„  ì €ì¥ (ìœ í‹¸ í˜¸ì¶œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€
                subj_dir = save_subject_curves( # í•œ í”¼í—˜ì(subj)ì— ëŒ€í•œ loss/acc ê³¡ì„  4ê°œë¥¼ ì €ì¥
                    base_dir=base_dir,
                    subj=subj,
                    moe_experts=moe_experts,
                    num_epochs=num_epochs,
                    train_acc=train_acc,
                    train_loss=train_loss,
                    test_acc_hist=test_acc_hist,
                    test_loss_hist=test_loss_hist,
                )
                print(f"[SAVE PLOT] saved curves to {subj_dir}")

                save_subject_taskwise_loss_curves( # í•œ í”¼í—˜ì(subj)ì— ëŒ€í•´ task ë³„ test loss curve ì €ì¥
                    subj_dir=subj_dir,
                    subj=subj,
                    moe_experts=moe_experts,
                    num_epochs=num_epochs,
                    te_task_loss_hist=te_task_loss_hist,
                    task_names=TASK_NAMES,
                )

                # â˜… ì´ subjectì˜ ì—í¬í¬ë³„ task accë¥¼ ì „ì—­ ë²„í¼ì— ëˆ„ì 
                for epoch_idx, (task_acc_dict, task_cnt_dict, task_loss_dict) in enumerate(
                        zip(te_task_acc_hist, te_task_count_hist, te_task_loss_hist)
                ):
                    for t, acc in task_acc_dict.items():
                        n_t = task_cnt_dict.get(t, 0)
                        if n_t > 0:
                            # â˜… acc í‰ê· ìš©
                            task_epoch_acc_sum[t][epoch_idx] += acc
                            task_epoch_subj_cnt[t][epoch_idx] += 1

                            # â˜… loss í‰ê· ìš©
                            loss_t = task_loss_dict.get(t, None)
                            if loss_t is not None:
                                task_epoch_loss_sum[t][epoch_idx] += loss_t
                                task_epoch_loss_subj_cnt[t][epoch_idx] += 1

                # --------- best model ë¡œë“œ & ìµœì¢… í‰ê°€ ---------
                best_path = r'C:\Users\User\PycharmProjects\Samsung_2024\All_in_one\best_model.pth'
                model.load_state_dict(torch.load(best_path))

                # ========= MoE expert í†µê³„ ì¼œê¸° ========= condition í†µí•© ìš© ì—¬ë¶€ì— ë”°ë¼ on/off
                for br in model.branches.values():
                    br.moe.track_stats = True
                    br.moe.reset_stats()
                # =======================================

                total_acc, task_acc, task_count, preds, targets, task_ids_all = test_bin_cls(
                    model, tst_loader=test_loader
                )
                ts_acc.append(total_acc)

                # â˜… subject Ã— task accuracy / sample ìˆ˜ ê¸°ë¡
                for t, acc in task_acc.items():
                    per_subj_task_acc[subj, t] = acc  # ì´ subjectì˜ task t ì •í™•ë„(%)
                    per_subj_task_n[subj, t] = task_count[t]  # ì´ subjectì˜ task t ìƒ˜í”Œ ìˆ˜



                # â˜… ì—¬ê¸°ì„œ task-wise ì •ë‹µ/ìƒ˜í”Œ ëˆ„ì  (ìš”ì²­ì‚¬í•­ 3)
                global_expert_hist, global_token_hist, global_stream_names, \
                    per_subj_expert_hist, per_subj_token_hist = process_subject_after_test( # í”¼í—˜ìë³„ expert ratio í”Œë¡¯ + global í†µê³„ ì—…ë°ì´íŠ¸
                    subj=subj,
                    moe_experts=moe_experts,
                    model=model,
                    valid_task_ids=valid_task_ids,
                    subj_dir=subj_dir,
                    total_acc=total_acc,
                    train_acc=train_acc,
                    train_loss=train_loss,
                    task_acc=task_acc,
                    task_count=task_count,
                    global_task_correct=global_task_correct,
                    global_task_total=global_task_total,
                    global_expert_hist=global_expert_hist,
                    global_token_hist=global_token_hist,
                    global_stream_names=global_stream_names,
                    task_names=TASK_NAMES,
                    num_subj=num_subj,
                    per_subj_expert_hist=per_subj_expert_hist,
                    per_subj_token_hist=per_subj_token_hist,
                )

            # ============= ì „ì²´ í”¼í—˜ì í‰ê·  (subj_mean) =============
            if len(used_subjects) == 0:
                print(f"[WARN] streams={cond_tag}, moe_experts={moe_experts}: "
                      f"no valid subjects, skip summary.")
                continue

            mean_dir = save_mean_curves_and_subject_acc(
                base_dir=base_dir,
                moe_experts=moe_experts,
                num_epochs=num_epochs,
                all_tr_acc=all_tr_acc,
                all_tr_loss=all_tr_loss,
                all_te_acc=all_te_acc,
                all_te_loss=all_te_loss,
                ts_acc=ts_acc,
                used_subjects=used_subjects,
            )

            # ----- taskë³„ epoch-mean accuracy ê³¡ì„  ì €ì¥ -----
            save_taskwise_epoch_mean_curves(
                mean_dir=mean_dir,
                moe_experts=moe_experts,
                num_epochs=num_epochs,
                task_epoch_acc_sum=task_epoch_acc_sum,
                task_epoch_subj_cnt=task_epoch_subj_cnt,
                task_epoch_loss_sum=task_epoch_loss_sum,
                task_epoch_loss_subj_cnt=task_epoch_loss_subj_cnt,
                task_names=TASK_NAMES,
            )
            
            # ----------------- subj_mean expert_ratio -----------------
            save_global_expert_ratio_plots(
                mean_dir=mean_dir,
                moe_experts=moe_experts,
                global_expert_hist=global_expert_hist,
                global_token_hist=global_token_hist,
                stream_names=global_stream_names,
                task_names=TASK_NAMES,
            )

            # ======= taskë³„ expert total count í”Œë¡¯ =======
            save_task_expert_total_counts(
                mean_dir=mean_dir,
                moe_experts=moe_experts,
                global_expert_hist=global_expert_hist,
                global_token_hist=global_token_hist,
                task_names=TASK_NAMES,
            )

            # --- accuracy ìƒ/ì¤‘/í•˜ ê·¸ë£¹ë³„ expert ë¹„ìœ¨ í”Œë¡¯ ---
            save_group_expert_ratio_plots(
                mean_dir=mean_dir,
                moe_experts=moe_experts,
                per_subj_expert_hist=per_subj_expert_hist,
                per_subj_token_hist=per_subj_token_hist,
                per_subj_task_acc=per_subj_task_acc,  # â˜… ë³€ê²½
                per_subj_task_n=per_subj_task_n,  # â˜… ë³€ê²½
                ts_acc=ts_acc,
                stream_names=global_stream_names,
                task_names=TASK_NAMES,
            )

            # --- taskë³„ subjectÃ—expert íˆíŠ¸ë§µ ---
            save_subject_expert_heatmaps(
                mean_dir=mean_dir,
                moe_experts=moe_experts,
                per_subj_expert_hist=per_subj_expert_hist,
                per_subj_task_acc=per_subj_task_acc,  # â˜… ë³€ê²½
                per_subj_task_n=per_subj_task_n,  # â˜… ë³€ê²½
                task_names=TASK_NAMES,
            )

            # í”¼í—˜ì í‰ê·  ë° task-wise í‰ê·  ì •í™•ë„ ì¶œë ¥
            print_taskwise_stats(
                cond_tag=cond_tag,
                moe_experts=moe_experts,
                ts_acc=ts_acc,
                global_task_correct=global_task_correct,
                global_task_total=global_task_total,
                task_names=TASK_NAMES,
            )

            # ----------------- Excelë¡œ ê²°ê³¼ ì €ì¥ -----------------
            save_summary_excel(
                mean_dir=mean_dir,
                moe_experts=moe_experts,
                cond_tag=cond_tag,
                ts_acc=ts_acc,
                used_subjects=used_subjects,  # â˜… ì—¬ê¸°ë¡œ êµì²´
                global_task_correct=global_task_correct,
                global_task_total=global_task_total,
                task_names=TASK_NAMES,
            )


if __name__ == "__main__":
    main()